# -*- coding: utf-8 -*-
"""Untitled.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kV8e01eYgt2ecuB6yHfDJxm2iIyLlEg7
"""

import pandas as pd
import numpy as np
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')

import os

# Change this to your folder in Drive
folder_path = '/content/drive/MyDrive'

# List all files and folders in this directory
for item in os.listdir(folder_path):
    print(item)

import os

folder_path = '/content/drive/MyDrive/eth-transactions-2025-08-24.csv'
files = os.listdir(folder_path)

print("Files in folder:", files)

data = pd.read_csv("/content/drive/MyDrive/eth-transactions-2025-08-24.csv/2025-08-24.csv")
#pd.set_option('display.max_rows', None)
data.head()

"""# Data Preprocessing"""

data.drop(axis=1, columns=["hash", "chain_id", "from", "to", "sources", "data_4bytes", "included_at_block_height"], inplace=True)
data.head()

"""## Ensure the dtype of data["value"] is not Object"""

data["value"] = pd.to_numeric(data["value"], errors="coerce").astype("Float64")
print(data["value"].dtype)

"""## Reorder the columns"""

cols = ["gas_price", "gas", "gas_tip_cap", "gas_fee_cap", "value", "nonce", "data_size", "tx_type", "timestamp_ms", "included_block_timestamp_ms", "inclusion_delay_ms"]
data = data[cols]
data.head()

"""### Remove those rows where the `inclusion_delay_ms` is less than 0"""

data.drop(data[data["inclusion_delay_ms"] < 0].index, axis=0, inplace=True)
data.reset_index(drop=True, inplace=True)
data.head(10)

"""### Rename the cols as per the paper"""

data.rename({'gas':"gas_limit"}, inplace=True) # didn't change the name, why??
data.head(), data.columns

"""### columns in csv:
1. timestamp_ms: Time when the Tx initiated
2. Value: Value sent in Wei
3. Nonce: Makes the Tx wait until the previous nonce Tx is confirmed (**values need transformation**)
4. gas: gas limit => max amount of gas someone's willing to pay
5. gas_price: price per unit gas
6. gas_tip_cap:
7. gas_fee_cap:
8. data_size: ?
9. timestamp_ms: Tx first seen at
10. included_block_timestamp_ms: Tx first mined at
11. inclusion_delay_ms: Tx ETA (**y values**)

Here, `inclusion_delay_ms = included_block_timestamp_ms - timestamp_ms`

## timestamp_0
timestamp_0 is the time when the Tx is seen in the mempool. Here we are not provided with the exact  
timestamp_0, so we can calculate it approximately by the formula:  
<div style="padding-left:100px"> timestamp_0 = included_block_timestamp_ms − inclusion_delay_ms </div>

## timestamp_1
timestamp_1 is the time when the Tx was confirmed by the mining node

## timestamp_1 - timestamp_0 -> To Predict

### Remove the `timestamp_ms` and `included_block_timestamp_ms` cols, as we don't have that info during inferencing
"""

data.drop(axis=1, columns=["timestamp_ms", "included_block_timestamp_ms"], inplace=True)
data.head()

"""# Exploratory Data Analysis"""

print(f"number of duplicate rows: {data.duplicated().sum()}")
data = data.drop_duplicates()
print(f"number of duplicate rows: {data.duplicated().sum()}")

# counter = 0
# for i in range(data.shape[0]):
#     if counter <= 10 and data.iloc[i, :].equals(data.iloc[i+1, :]):
#         print(f"row={i}", data.iloc[i, :])
#         print('*'*100)
#         counter += 1
data.head()

data.describe().T

"""### Some rows have gas, gas_price,... = 0 (impossible)"""

print(f"Number of incorrect gas_price = {(data["gas_price"] <= 0).sum()}")
print(f"Number of incorrect gas = {(data["gas"] <= 0).sum()}")
print(f"Number of incorrect gas_tip_cap = {(data["gas_tip_cap"] <= 0).sum()}")
print(f"Number of incorrect gas_fee_cap = {(data["gas_fee_cap"] <= 0).sum()}")

data.drop(axis=0,
          index=data[
              (data["gas_price"] <= 0)
              | (data["gas"] <= 0)
              | (data["gas_tip_cap"] <= 0)
              | (data["gas_fee_cap"] <= 0)
          ].index, inplace=True)
print(f"Number of incorrect gas_price = {(data["gas_price"] <= 0).sum()}")
print(f"Number of incorrect gas = {(data["gas"] <= 0).sum()}")
print(f"Number of incorrect gas_tip_cap = {(data["gas_tip_cap"] <= 0).sum()}")
print(f"Number of incorrect gas_fee_cap = {(data["gas_fee_cap"] <= 0).sum()}")

data.shape

data.describe().T

"""### Need to Scale"""

data.head()

data["inclusion_delay_ms"]

"""### Some features are heavily tailed: `value`, `gas_price`, `gas_tip_cap`, `gas_fee_cap`, `nonce`
-> Use the log scaling, then apply StandardScaler

### `gas`, `data_size`
-> Directly apply StandardScaler

### One-Hot Encode `tx_type`
"""

data.head()

import matplotlib.pyplot as plt

# Plot histograms for the heavily tailed features without seaborn
features = ['value', 'gas_price', 'gas_tip_cap', 'gas_fee_cap', 'nonce']

plt.figure(figsize=(15, 10))

for i, col in enumerate(features, 1):
    plt.subplot(2, 3, i)
    plt.hist(data[col].dropna(), bins=100)
    plt.title(f"Histogram of {col}")
    plt.xlabel(col)
    plt.ylabel("Frequency")

plt.tight_layout()
plt.show()

data[data["inclusion_delay_ms"] == data["inclusion_delay_ms"].max()]
# These transactions took the longest 27-28 hrs to be confirmed (why??)

from sklearn.preprocessing import StandardScaler

features = ['value', 'gas_price', 'gas_tip_cap', 'gas_fee_cap', 'nonce']
plt.figure(figsize=(15, 10))
for idx, feature in enumerate(features, 1):
    plt.subplot(2, 3, idx)
    data[feature] = np.log1p(data[feature])
    data[feature] = StandardScaler().fit_transform(data[feature].values.reshape(-1, 1))
    plt.hist(data[feature], bins=100)
    plt.title(f"Histogram of {feature}")

plt.tight_layout()
plt.show()

data["gas"] = StandardScaler().fit_transform(data["gas"].values.reshape(-1, 1))
data["data_size"] = StandardScaler().fit_transform(data["data_size"].values.reshape(-1, 1))

data.head()

data.describe().T

data = pd.get_dummies(data, columns=["tx_type"], dtype=int)

data = data[[col_name for col_name in data.columns if col_name != 'inclusion_delay_ms'] + ['inclusion_delay_ms']]
data.head()

data.describe().T

data["inclusion_delay_ms"].describe().T

print(f"Before removing: {len(data[data["inclusion_delay_ms"] == 0])}")
data = data[data['inclusion_delay_ms'] != 0]
print(f"After removing: {len(data[data["inclusion_delay_ms"] == 0])}")

from sklearn.model_selection import train_test_split
y = data["inclusion_delay_ms"]
data = data.iloc[:, :-1]

import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

feature = "inclusion_delay_ms"
plt.figure(figsize=(15, 10))
plt.subplot(3, 2, 1)
plt.hist(y, bins=100)
plt.title(f"Histogram of {feature} (before)")

y_log = np.log1p(y)
scaler_y = StandardScaler()
y_scaled = pd.Series(
    scaler_y.fit_transform(y_log.values.reshape(-1, 1)).ravel(),
    index=y.index,
    name=feature
)
plt.subplot(3, 2, 2)
plt.hist(y_scaled, bins=100)
plt.title(f"Histogram of {feature} (after)")

plt.show()

"""# Training -- Random Forest"""

type(y_scaled)

X_train, X_test, y_train, y_test = train_test_split(data, y_scaled, test_size=0.2, random_state=42)

y_train

"""250 trees/5 samples"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np

rf = RandomForestRegressor(
    n_estimators=500,
    max_depth=10,
    random_state=42,
    n_jobs=-1,
    verbose=1   # prints progress
)

rf.fit(X_train, y_train)


# Predict on test set
y_pred = rf.predict(X_test)

# Metrics
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mape = np.mean(np.abs((y_test - y_pred) / np.where(y_test==0, 1, y_test))) * 100

print(f"MAE: {mae:.3f}, RMSE: {rmse:.3f}, MAPE: {mape:.2f}%")

import joblib

# Example path in your Drive
path = "/content/drive/MyDrive/random_forest_500.pkl"

# Save the model
joblib.dump(rf, path)
print(f"Model saved to {path}")

X_train.shape

y_train.describe()

# y_test_scaled: true values
# y_pred_scaled: predicted values

tolerance = 0.20  # 20%
pred_acc = np.mean(np.isclose(y_test, y_pred, rtol=tolerance)) * 100

print(f"Pred (0.20): {pred_acc:.2f}%")

def reverseY(y_scaled):
    # ensure numpy array
    y_scaled = np.array(y_scaled).reshape(-1, 1)

    # undo standardization
    y_log = scaler_y.inverse_transform(y_scaled).ravel()

    # undo log1p
    y_original = np.expm1(y_log)
    return y_original

y_pred_scaled = rf.predict(X_test)
print(y_pred_scaled)
y_pred = reverseY(y_pred_scaled)
y_test = reverseY(y_test)

y_pred = y_pred/1e3

y_test = y_test/1e3

print(f"y_pred = {y_pred}")
print(f"y_test = {y_test}")

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# MAE (Mean Absolute Error)
mae = mean_absolute_error(y_test, y_pred)

# RMSE (Root Mean Squared Error)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

# R² (Coefficient of Determination)
r2 = r2_score(y_test, y_pred)

print("MAE:", mae)
print("RMSE:", rmse)
print("R²:", r2)

